{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# reload modules in .py files\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "e7g08cqjJ0ti"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pull repo\n",
        "!git clone https://github.com/char-tan/sparsity"
      ],
      "metadata": {
        "id": "bRaMT-oNJvfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19debd7a-1ac6-478e-9a8a-2fd28f897eed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sparsity'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 61 (delta 27), reused 42 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directory\n",
        "import os\n",
        "os.chdir('sparsity/ops')"
      ],
      "metadata": {
        "id": "nrV0RsgRJ2xZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "id": "Pb3TRf5knTqP",
        "outputId": "2c2a8294-3c92-40b6-d633-0d54c69a4a22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing ops_cpp.egg-info/PKG-INFO\n",
            "writing dependency_links to ops_cpp.egg-info/dependency_links.txt\n",
            "writing top-level names to ops_cpp.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'ops_cpp.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "building 'ops_cpp' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c ops_cuda.cpp -o build/temp.linux-x86_64-3.8/ops_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=ops_cpp -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c ops_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/ops_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=ops_cpp -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "ops_cuda_kernel.cu(24): error: expected an expression\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \")\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: no instance of function template \"cuda_add_kernel\" matches the argument list\n",
            "            argument types are: (c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Double> *, c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Double> *, c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Double> *, <error-type>)\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \";\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \";\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected an expression\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \")\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: no instance of function template \"cuda_add_kernel\" matches the argument list\n",
            "            argument types are: (c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Float> *, c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Float> *, c10::impl::ScalarTypeToCPPTypeT<c10::ScalarType::Float> *, <error-type>)\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \";\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: deduced return type \"lambda []()->void\" conflicts with previously deduced type \"lambda []()->void\"\n",
            "\n",
            "ops_cuda_kernel.cu(24): error: expected a \";\"\n",
            "\n",
            "ops_cuda_kernel.cu(32): error: expected a \";\"\n",
            "\n",
            "12 errors detected in the compilation of \"ops_cuda_kernel.cu\".\n",
            "error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using -e has caveats: https://stackoverflow.com/a/20043907\n",
        "!pip3 install ."
      ],
      "metadata": {
        "id": "cXhwtpo1W7-Z",
        "outputId": "5eb752ce-a1d8-468c-c7a9-01add827a4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/sparsity/ops\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ops-cpp\n",
            "  Building wheel for ops-cpp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ops-cpp: filename=ops_cpp-0.0.0-cp38-cp38-linux_x86_64.whl size=2407560 sha256=6a713393dfd7a9fe6282f56489796e06ef9d373ef78b2d939a559fdfd0e9a5c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ebuygvg3/wheels/a0/60/38/cf329a695010b267296cd00c31c2a180e9d57ca424280b3d28\n",
            "Successfully built ops-cpp\n",
            "Installing collected packages: ops-cpp\n",
            "  Attempting uninstall: ops-cpp\n",
            "    Found existing installation: ops-cpp 0.0.0\n",
            "    Uninstalling ops-cpp-0.0.0:\n",
            "      Successfully uninstalled ops-cpp-0.0.0\n",
            "Successfully installed ops-cpp-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample usage of our extension"
      ],
      "metadata": {
        "id": "wyGQyCVSY-3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os, sys\n",
        "sys.path.append(os.getcwd())\n",
        "import ops_cpp\n",
        "\n",
        "a = torch.rand([3,3]).cuda()\n",
        "b = a\n",
        "c = ops_cpp.add(a, b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0_qmycdK_Qu",
        "outputId": "b397fefa-917a-466a-85bd-ea434429195d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6803, 1.8560, 1.7057],\n",
            "        [0.9444, 1.2852, 1.8762],\n",
            "        [0.1864, 0.8852, 0.7403]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git diff"
      ],
      "metadata": {
        "id": "XsCp-_rgo9Ph",
        "outputId": "29a9ed67-cd60-4956-d35a-4905103e2cf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mdiff --git a/ops/ops_cuda_kernel.cu b/ops/ops_cuda_kernel.cu\u001b[m\n",
            "\u001b[1mindex 092ed31..2958650 100644\u001b[m\n",
            "\u001b[1m--- a/ops/ops_cuda_kernel.cu\u001b[m\n",
            "\u001b[1m+++ b/ops/ops_cuda_kernel.cu\u001b[m\n",
            "\u001b[36m@@ -4,10 +4,33 @@\u001b[m\n",
            " #include <cuda_runtime.h>\u001b[m\n",
            " \u001b[m\n",
            " \u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32mtemplate <typename scalar_t>\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m__global__ void cuda_add_kernel(\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    scalar_t* __restrict__ a,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    scalar_t* __restrict__ b,\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    scalar_t* __restrict__ c\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    ) {\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        c = a + b\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m      }\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            " torch::Tensor cuda_add(\u001b[m\n",
            " \t\ttorch::Tensor a,\u001b[m\n",
            " \t\ttorch::Tensor b)\u001b[m\n",
            " {\u001b[m\n",
            "\u001b[31m-\tauto c = torch::add(a, b);\u001b[m\n",
            "\u001b[31m-\treturn c;\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m  auto c = torch::zeros_like(a);\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m  AT_DISPATCH_FLOATING_TYPES(a.type(), \"cuda_add\", ([&] {\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m    cuda_add_kernel<scalar_t><<<1, 1>>>(\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        a.data<scalar_t>(),\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        b.data<scalar_t>(),\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m        c.data<scalar_t>(),\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m  })));\u001b[m\n",
            "\u001b[32m+\u001b[m\n",
            "\u001b[32m+\u001b[m\u001b[32m  return {c}\u001b[m\n",
            " }\u001b[m\n",
            "\u001b[41m+\u001b[m\n",
            "\u001b[41m+\u001b[m\n",
            "\u001b[41m+\u001b[m\n",
            "\u001b[41m+\u001b[m\n"
          ]
        }
      ]
    }
  ]
}